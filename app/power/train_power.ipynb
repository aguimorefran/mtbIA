{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, BatchNormalization, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import RNN, LSTMCell\n",
    "import optuna\n",
    "from optuna.integration import TFKerasPruningCallback\n",
    "import gc\n",
    "import subprocess\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "RAW_DATA_PATH = \"../data/activity_data_raw.csv\"\n",
    "PLANNED_WORKOUTS_PATH = \"../data/planned_workouts.csv\"\n",
    "TARGET_FEATURES = [\"power_5s_avg\"]\n",
    "FEATURES = [\"grade\", \"ascent_meters\", \"distance_meters\", \"atl_start\", \"ctl_start\", \"watt_kg\", \"temperature\", \"distance_diff\", \"grade_5s_avg\", \"grade_diff\"]\n",
    "ALL_FEATURES = TARGET_FEATURES + FEATURES\n",
    "MODEL_PATH = \"../models/LSTM_power.keras\"\n",
    "METRICS_PATH = \"../data/model_metrics_power.csv\"\n",
    "\n",
    "def load_activity_data(csv_path):\n",
    "    logger.info(f\"Loading data from {csv_path}\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "    logger.info(\"Data loaded successfully\")\n",
    "    return df\n",
    "\n",
    "def ensure_dir(file_path):\n",
    "    directory = os.path.dirname(file_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "def preprocess_data(df, drop_activities_with_workout=False):\n",
    "    logger.info(\"Starting data preprocessing\")\n",
    "    df = df.sort_values(by=[\"activity_id\", \"timestamp\"])\n",
    "    df[\"altitude_diff\"] = df[\"altitude\"].diff()\n",
    "    df[\"distance_diff\"] = df[\"distance\"].diff()\n",
    "    df[\"ascent_meters\"] = df[\"altitude_diff\"].apply(lambda x: x if x > 0 else 0).cumsum()\n",
    "    df[\"grade_diff\"] = df[\"grade\"].diff().fillna(0)\n",
    "    df[\"seconds_from_start\"] = (df[\"timestamp\"] - df[\"timestamp\"].iloc[0]).dt.total_seconds()\n",
    "    df.rename(columns={\"distance\": \"distance_meters\"}, inplace=True)\n",
    "    \n",
    "    df.fillna(method='ffill', inplace=True)\n",
    "    df.fillna(method='bfill', inplace=True)\n",
    "\n",
    "    df[\"power_5s_avg\"] = df[\"power\"].rolling(window=5, min_periods=1).mean()\n",
    "    df[\"grade_5s_avg\"] = df[\"grade\"].rolling(window=5, min_periods=1).mean()\n",
    "\n",
    "    planned_workouts = pd.read_csv(PLANNED_WORKOUTS_PATH)\n",
    "\n",
    "    if drop_activities_with_workout:\n",
    "        intersecting_activities = planned_workouts[\"paired_activity_id\"].unique()\n",
    "        logger.info(f\"Dropping {len(intersecting_activities)} activities with planned workouts\")\n",
    "        df = df[~df[\"activity_id\"].isin(intersecting_activities)]\n",
    "        logger.info(f\"Remaining activities: {df['activity_id'].nunique()}\")\n",
    "\n",
    "    rows_na = df.isna().sum().sum()\n",
    "    df.dropna(inplace=True)\n",
    "    logger.info(f\"Dropped {rows_na} rows with NaN values\")\n",
    "    if df[ALL_FEATURES].isnull().values.any() or np.isinf(df[ALL_FEATURES]).values.any():\n",
    "        logger.error(\"NaN or infinite values found in the data\")\n",
    "        raise ValueError(\"NaN or infinite values found in the data\")\n",
    "\n",
    "    logger.info(\"Data preprocessing completed\")\n",
    "    return df\n",
    "\n",
    "def split_by_activities(df, test_size=0.2, val_size=0.1):\n",
    "    activity_ids = df['activity_id'].unique()\n",
    "    train_ids, test_ids = train_test_split(activity_ids, test_size=test_size, random_state=42)\n",
    "    train_ids, val_ids = train_test_split(train_ids, test_size=val_size / (1 - test_size), random_state=42)\n",
    "    \n",
    "    train_df = df[df['activity_id'].isin(train_ids)]\n",
    "    val_df = df[df['activity_id'].isin(val_ids)]\n",
    "    test_df = df[df['activity_id'].isin(test_ids)]\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "def scale_data(train, val, test, scaler_type=\"StandardScaler\"):\n",
    "    if scaler_type == \"StandardScaler\":\n",
    "        scaler = StandardScaler()\n",
    "    elif scaler_type == \"MinMaxScaler\":\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "    features = FEATURES + TARGET_FEATURES\n",
    "\n",
    "    train_scaled = train.copy()\n",
    "    val_scaled = val.copy()\n",
    "    test_scaled = test.copy()\n",
    "    \n",
    "    scaler.fit(train[features])\n",
    "    \n",
    "    train_scaled[features] = scaler.transform(train[features])\n",
    "    val_scaled[features] = scaler.transform(val[features])\n",
    "    test_scaled[features] = scaler.transform(test[features])\n",
    "    \n",
    "    return train_scaled, val_scaled, test_scaled\n",
    "\n",
    "def augment_data(seq, aug_methods):\n",
    "    method = np.random.choice(aug_methods)\n",
    "    if method == 'noise':\n",
    "        noise = np.random.normal(0, 0.01, seq.shape)\n",
    "        return seq + noise\n",
    "    elif method == 'scaling':\n",
    "        factor = np.random.uniform(0.9, 1.1)\n",
    "        return seq * factor\n",
    "    elif method == 'shifting':\n",
    "        shift = np.random.randint(-3, 3)\n",
    "        return np.roll(seq, shift, axis=0)\n",
    "    elif method == 'time_warping':\n",
    "        time_steps = np.arange(seq.shape[0])\n",
    "        warp = np.interp(time_steps, time_steps, seq)\n",
    "        return warp\n",
    "    return seq\n",
    "\n",
    "def create_sequences(data, sequence_length, target_columns, augmentation=False):\n",
    "    num_records = len(data)\n",
    "    num_features = len(FEATURES)\n",
    "    num_targets = len(target_columns)\n",
    "\n",
    "    sequences = np.zeros((num_records - sequence_length, sequence_length, num_features), dtype=np.float32)\n",
    "    targets = np.zeros((num_records - sequence_length, num_targets), dtype=np.float32)\n",
    "\n",
    "    feature_data = data[FEATURES].values\n",
    "    target_data = data[target_columns].values\n",
    "\n",
    "    for i in range(num_records - sequence_length):\n",
    "        seq = feature_data[i:i + sequence_length]\n",
    "        if augmentation:\n",
    "            seq = augment_data(seq, ['noise', 'scaling', 'shifting'])\n",
    "        sequences[i] = seq\n",
    "        targets[i] = target_data[i + sequence_length]\n",
    "\n",
    "    return sequences, targets\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "def build_model(\n",
    "        learning_rate=0.001,\n",
    "        cnn_filters=(),\n",
    "        dense_units=(),\n",
    "        lstm_units=(),\n",
    "        sequence_length=60,\n",
    "        dropout_rate_dense=0, \n",
    "        dropout_rate_lstm=0,\n",
    "        dropout_rate_cnn=0,\n",
    "        add_batch_norm=False,\n",
    "        l2_reg=0.01,\n",
    "):\n",
    "    model = Sequential()\n",
    "\n",
    "    # CNN LAYERS\n",
    "    if cnn_filters:\n",
    "        for i, num_filters in enumerate(cnn_filters):\n",
    "            if i == 0:\n",
    "                model.add(Conv1D(filters=num_filters, kernel_size=3, activation='relu', input_shape=(sequence_length, len(FEATURES))))\n",
    "            else:\n",
    "                model.add(Conv1D(filters=num_filters, kernel_size=3, activation='relu'))\n",
    "            model.add(MaxPooling1D(pool_size=2))\n",
    "            model.add(Dropout(dropout_rate_cnn))\n",
    "        if add_batch_norm:\n",
    "            model.add(BatchNormalization())\n",
    "\n",
    "    # LSTM LAYERS\n",
    "    for i, num_units in enumerate(lstm_units):\n",
    "        return_seq = True if i < len(lstm_units) - 1 else False\n",
    "        model.add(RNN(LSTMCell(num_units, activation='tanh', kernel_regularizer=l2(l2_reg)), return_sequences=return_seq))        \n",
    "        if add_batch_norm:\n",
    "            model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout_rate_lstm))\n",
    "\n",
    "    # DENSE LAYERS\n",
    "    for num_units in dense_units:\n",
    "        model.add(Dense(num_units, activation='relu', kernel_regularizer=l2(l2_reg)))\n",
    "        if add_batch_norm:\n",
    "            model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout_rate_dense))\n",
    "\n",
    "    model.add(Dense(len(TARGET_FEATURES)))\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss=\"mse\")\n",
    "    \n",
    "    logger.info(\"LSTM model built and compiled\")\n",
    "    return model\n",
    "\n",
    "augmentation_methods = ['noise', 'scaling', 'shifting', 'time_warping']\n",
    "augmentation_combinations = []\n",
    "for i in range(1, len(augmentation_methods) + 1):\n",
    "    augmentation_combinations.extend(itertools.combinations(augmentation_methods, i))\n",
    "\n",
    "def objective(trial):\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    sequence_length = trial.suggest_int('sequence_length', 10, 120)\n",
    "    lstm_units = trial.suggest_categorical('lstm_units', \n",
    "                                           ((128,), (128, 128), (128, 64), (64, 32), (64, 64), (32, 32),\n",
    "                                            (50, 50, 50), (100, 100, 100), (128, 64, 32), (64, 32, 16),\n",
    "                                            (128, 128, 64, 32), (64, 64, 32, 16),\n",
    "                                            (30, 30, 30, 30, 30), (50, 50, 50, 50, 50), (100, 100, 100, 100, 100)))\n",
    "    dense_units = trial.suggest_categorical('dense_units', \n",
    "                                            ((32, 16), (64, 32), (128, 64), (64, 64), (32, 32),\n",
    "                                             (128, 64, 32), (64, 32, 16), (128, 64, 32),\n",
    "                                                (128, 128, 64, 32), (64, 64, 32, 16),\n",
    "                                                (128, 128, 64, 64), (64, 64, 32, 32)))\n",
    "    dropout_rate_lstm = trial.suggest_float('dropout_rate_lstm', 0.0, 0.5)\n",
    "    dropout_rate_dense = trial.suggest_float('dropout_rate_dense', 0.0, 0.5)\n",
    "    dropout_rate_cnn = trial.suggest_float('dropout_rate_cnn', 0.0, 0.5)\n",
    "    l2_reg = trial.suggest_float('l2_reg', 0.0, 0.1)\n",
    "    cnn_filters = trial.suggest_categorical('cnn_filters', ((), (32, 64), (64, 128), (128, 64), (64, 32)))\n",
    "    augmentation = trial.suggest_categorical('augmentation_methods', augmentation_combinations)\n",
    "    scaler_type = trial.suggest_categorical('scaler_type', ('StandardScaler', 'MinMaxScaler'))\n",
    "\n",
    "    model = build_model(\n",
    "        learning_rate=learning_rate,\n",
    "        cnn_filters=cnn_filters,\n",
    "        lstm_units=lstm_units,\n",
    "        dense_units=dense_units,\n",
    "        dropout_rate_lstm=dropout_rate_lstm,\n",
    "        dropout_rate_dense=dropout_rate_dense,\n",
    "        dropout_rate_cnn=dropout_rate_cnn,\n",
    "        l2_reg=l2_reg,\n",
    "        sequence_length=sequence_length\n",
    "    )\n",
    "    \n",
    "    batch_size = 256\n",
    "    checkpoint_callback = ModelCheckpoint(filepath=f'checkpoint_trial_{trial.number}.h5', save_weights_only=True, save_best_only=True, monitor='val_loss', verbose=1)\n",
    "    \n",
    "    train_scaled, val_scaled, test_scaled = scale_data(train_df, val_df, test_df, scaler_type)\n",
    "\n",
    "    train_sequences, train_targets_seq = create_sequences(train_scaled, sequence_length, TARGET_FEATURES, augmentation)\n",
    "    val_sequences, val_targets_seq = create_sequences(val_scaled, sequence_length, TARGET_FEATURES)\n",
    "\n",
    "    history = model.fit(train_sequences, train_targets_seq, batch_size=batch_size, epochs=50, validation_data=(val_sequences, val_targets_seq), callbacks=[\n",
    "        TFKerasPruningCallback(trial, 'val_loss'),\n",
    "        LearningRateScheduler(scheduler),\n",
    "        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "        checkpoint_callback\n",
    "    ], verbose=0)\n",
    "    \n",
    "    val_loss = min(history.history['val_loss'])\n",
    "    del model\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    \n",
    "    return val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_activity_data(RAW_DATA_PATH)\n",
    "df = preprocess_data(df, drop_activities_with_workout=True)\n",
    "train_df, val_df, test_df = split_by_activities(df)\n",
    "\n",
    "def run_dashboard(storage_name):\n",
    "    subprocess.Popen([\"optuna-dashboard\", storage_name])\n",
    "\n",
    "study_name = \"example_study\"  \n",
    "storage_name = \"sqlite:///example_study.db\"  \n",
    "\n",
    "run_dashboard(storage_name)\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "study = optuna.create_study(study_name=study_name, direction='minimize', storage=storage_name, load_if_exists=True)\n",
    "\n",
    "try:\n",
    "    study.optimize(objective, n_trials=50)\n",
    "except Exception as e:\n",
    "    print(f\"Optimization failed with exception: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = study.best_trial\n",
    "results = {\n",
    "    'learning_rate': best_trial.params['learning_rate'],\n",
    "    'cnn_filters': best_trial.params['cnn_filters'],\n",
    "    'lstm_units': best_trial.params['lstm_units'],\n",
    "    'dense_units': best_trial.params['dense_units'],\n",
    "    'dropout_rate_lstm': best_trial.params['dropout_rate_lstm'],\n",
    "    'dropout_rate_dense': best_trial.params['dropout_rate_dense'],\n",
    "    'dropout_rate_cnn': best_trial.params['dropout_rate_cnn'],\n",
    "    'l2_reg': best_trial.params['l2_reg'],\n",
    "    'sequence_length': best_trial.params['sequence_length'],\n",
    "    'scaler_type': best_trial.params['scaler_type'],\n",
    "    'augmentation': best_trial.params['augmentation_methods'],\n",
    "    'val_loss': best_trial.value\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame([results])\n",
    "results_df.to_csv(METRICS_PATH, index=False)\n",
    "\n",
    "# Entrenar el modelo final con los mejores hiperparÃ¡metros\n",
    "model = build_model(\n",
    "    learning_rate=best_trial.params['learning_rate'],\n",
    "    cnn_filters=best_trial.params['cnn_filters'],\n",
    "    lstm_units=best_trial.params['lstm_units'],\n",
    "    dense_units=best_trial.params['dense_units'],\n",
    "    dropout_rate_lstm=best_trial.params['dropout_rate_lstm'],\n",
    "    dropout_rate_dense=best_trial.params['dropout_rate_dense'],\n",
    "    dropout_rate_cnn=best_trial.params['dropout_rate_cnn'],\n",
    "    l2_reg=best_trial.params['l2_reg'],\n",
    "    sequence_length=best_trial.params['sequence_length']\n",
    ")\n",
    "\n",
    "train_scaled, val_scaled, test_scaled = scale_data(train_df, val_df, test_df, best_trial.params['scaler_type'])\n",
    "\n",
    "if best_trial.params['augmentation']:\n",
    "    train_augmented_features = augment_data(train_scaled, best_trial.params['augmentation'])\n",
    "else:\n",
    "    train_augmented_features = train_scaled\n",
    "\n",
    "train_sequences, train_targets_seq = create_sequences(train_augmented_features, best_trial.params['sequence_length'], TARGET_FEATURES)\n",
    "val_sequences, val_targets_seq = create_sequences(val_scaled, best_trial.params['sequence_length'], TARGET_FEATURES)\n",
    "\n",
    "history = model.fit(train_sequences, train_targets_seq, batch_size=256, epochs=50, validation_data=(val_sequences, val_targets_seq), \n",
    "                    callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)],\n",
    "                    verbose=1)\n",
    "\n",
    "test_sequences, test_targets_seq = create_sequences(test_scaled, best_trial.params['sequence_length'], TARGET_FEATURES)\n",
    "test_loss = model.evaluate(test_sequences, test_targets_seq)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "model.save(MODEL_PATH)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfdml_plugin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
